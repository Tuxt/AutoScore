Using TensorFlow backend.
>>> Imports:
#coding=utf-8

try:
    from keras.models import Sequential
except:
    pass

try:
    from keras.layers import Dense, GRU
except:
    pass

try:
    from keras.callbacks import ModelCheckpoint, EarlyStopping
except:
    pass

try:
    import datamanager as dm
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas.distributions import choice, uniform, conditional
except:
    pass

try:
    import time
except:
    pass

try:
    import os
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'numgrulayers': hp.choice('numgrulayers', ['three']),
        'units': hp.choice('units', [32,64,128,256,512]),
        'units_1': hp.choice('units_1', [32,64,128,256,512]),
        'units_2': hp.choice('units_2', [32,64,128,256,512]),
        'units_3': hp.choice('units_3', [32,64,128,256,512]),
        'units_4': hp.choice('units_4', [32,64,128,256,512]),
        'units_5': hp.choice('units_5', [32,64,128,256,512]),
        'units_6': hp.choice('units_6', [32,64,128,256,512]),
        'units_7': hp.choice('units_7', [32,64,128,256,512]),
        'batch_size': hp.choice('batch_size', [32,64,128]),
    }

>>> Data
  1: 
  2: x_train = dm.load("dataset/X_train")
  3: y_train = dm.load("dataset/y_train")
  4: x_val = dm.load("dataset/X_val")
  5: y_val = dm.load("dataset/y_val")
  6: 
  7: 
  8: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     weights_dir = 'weights'
   4:     history_dir = 'history'
   5:     
   6:     numgrulayers = space['numgrulayers']
   7:     numgrulayers = conditional(numgrulayers)
   8: 
   9: 
  10:     model = Sequential()
  11:     model.add(GRU(units=space['units'], activation='tanh', input_shape=(x_train.shape[1],x_train.shape[2]), return_sequences=True ))
  12:     model.add(GRU(units=space['units_1'], activation='tanh', return_sequences=True ))
  13:     if numgrulayers == 'five':
  14:         model.add(GRU(units=space['units_2'], activation='tanh', return_sequences=True ))
  15:         model.add(GRU(units=space['units_3'], activation='tanh', return_sequences=True ))
  16:     elif numgrulayers == 'six':
  17:         model.add(GRU(units=space['units_4'], activation='tanh', return_sequences=True ))
  18:         model.add(GRU(units=space['units_5'], activation='tanh', return_sequences=True ))
  19:         model.add(GRU(units=space['units_6'], activation='tanh', return_sequences=True ))
  20:     model.add(GRU(units=space['units_7'], activation='tanh', return_sequences=False ))
  21:     model.add(Dense(y_train.shape[1], activation='softmax'))
  22:     model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
  23: 
  24:     if numgrulayers == 'three':
  25:         filename = weights_dir+'/' + \
  26:                 numgrulayers + '_' + \
  27:                 str(space['units']) + '_' + \
  28:                 str(space['units_1']) + '_' + \
  29:                 str(space['units_7']) + '_' + \
  30:                 str(space['batch_size']) + \
  31:                 '_{epoch:03d}_{loss:.4f}_{acc:.4f}_{val_loss:.4f}_{val_acc:.4f}.hdf5'
  32:     elif numgrulayers == 'five':
  33:         filename = weights_dir+'/' + \
  34:                 numgrulayers + '_' + \
  35:                 str(space['units']) + '_' + \
  36:                 str(space['units_1']) + '_' + \
  37:                 str(space['units_2']) + '_' + \
  38:                 str(space['units_3']) + '_' + \
  39:                 str(space['units_7']) + '_' + \
  40:                 str(space['batch_size']) + \
  41:                 '_{epoch:03d}_{loss:.4f}_{acc:.4f}_{val_loss:.4f}_{val_acc:.4f}.hdf5'
  42:     elif numgrulayers == 'six':
  43:         filename = weights_dir+'/' + \
  44:                 numgrulayers + '_' + \
  45:                 str(space['units']) + '_' + \
  46:                 str(space['units_1']) + '_' + \
  47:                 str(space['units_4']) + '_' + \
  48:                 str(space['units_5']) + '_' + \
  49:                 str(space['units_6']) + '_' + \
  50:                 str(space['units_7']) + '_' + \
  51:                 str(space['batch_size']) + \
  52:                 '_{epoch:03d}_{loss:.4f}_{acc:.4f}_{val_loss:.4f}_{val_acc:.4f}.hdf5'
  53: 
  54:     checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')
  55:     earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')
  56:     callbacks = [checkpoint,earlystopping]
  57:     
  58:     result = model.fit(x_train, y_train, batch_size=space['batch_size'], epochs=20,verbose=6, validation_data=(x_val, y_val), callbacks=callbacks)
  59: 
  60:     parameters = space
  61:     parameters['history'] = result.history
  62:     if numgrulayers == 'three':
  63:         dm.save(parameters, history_dir + '/' + \
  64:             numgrulayers + '_' + \
  65:             str(space['units']) + '_' + \
  66:             str(space['units_1']) + '_' + \
  67:             str(space['units_7']) + '_' + \
  68:             str(space['batch_size']))
  69:     elif numgrulayers == 'five':
  70:         dm.save(parameters, history_dir + '/' + \
  71:             numgrulayers + '_' + \
  72:             str(space['units']) + '_' + \
  73:             str(space['units_1']) + '_' + \
  74:             str(space['units_2']) + '_' + \
  75:             str(space['units_3']) + '_' + \
  76:             str(space['units_7']) + '_' + \
  77:             str(space['batch_size']))
  78:     elif numgrulayers == 'six':
  79:         dm.save(parameters, history_dir + '/' + \
  80:             numgrulayers + '_' + \
  81:             str(space['units']) + '_' + \
  82:             str(space['units_1']) + '_' + \
  83:             str(space['units_4']) + '_' + \
  84:             str(space['units_5']) + '_' + \
  85:             str(space['units_6']) + '_' + \
  86:             str(space['units_7']) + '_' + \
  87:             str(space['batch_size']))
  88: 
  89:     loss,acc = model.evaluate(x_val,y_val, verbose=0)
  90:     print("Test loss: "+str(loss)+"\tTest acc: " +str(acc))
  91:     return {'status': STATUS_OK, 'model':model, 'loss':loss, 'acc':acc }
  92: 
Train on 88944 samples, validate on 29649 samples
Epoch 1/20
2018-07-26 14:34:44.215510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-26 14:34:44.215855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1031] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.759
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.70GiB
2018-07-26 14:34:44.215878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
Epoch 00001: loss improved from inf to 2.45915, saving model to weights/three_64_64_64_32_001_2.4591_0.2618_2.2811_0.2873.hdf5
Epoch 2/20
Epoch 00002: loss improved from 2.45915 to 2.19463, saving model to weights/three_64_64_64_32_002_2.1946_0.3163_2.1452_0.3278.hdf5
Epoch 3/20
Epoch 00003: loss improved from 2.19463 to 2.07136, saving model to weights/three_64_64_64_32_003_2.0714_0.3499_2.0523_0.3530.hdf5
Epoch 4/20
Epoch 00004: loss improved from 2.07136 to 1.98462, saving model to weights/three_64_64_64_32_004_1.9846_0.3726_2.0062_0.3719.hdf5
Epoch 5/20
Epoch 00005: loss improved from 1.98462 to 1.92380, saving model to weights/three_64_64_64_32_005_1.9238_0.3901_1.9770_0.3825.hdf5
Epoch 6/20
Epoch 00006: loss improved from 1.92380 to 1.88035, saving model to weights/three_64_64_64_32_006_1.8803_0.4036_1.9201_0.3949.hdf5
Epoch 7/20
Epoch 00007: loss improved from 1.88035 to 1.84339, saving model to weights/three_64_64_64_32_007_1.8434_0.4146_1.9065_0.4025.hdf5
Epoch 8/20
Epoch 00008: loss improved from 1.84339 to 1.81382, saving model to weights/three_64_64_64_32_008_1.8138_0.4241_1.9040_0.4022.hdf5
Epoch 9/20
Epoch 00009: loss improved from 1.81382 to 1.78602, saving model to weights/three_64_64_64_32_009_1.7860_0.4325_1.8801_0.4087.hdf5
Epoch 10/20
Epoch 00010: loss improved from 1.78602 to 1.76763, saving model to weights/three_64_64_64_32_010_1.7676_0.4390_1.8699_0.4095.hdf5
Epoch 11/20
Epoch 00011: loss improved from 1.76763 to 1.74469, saving model to weights/three_64_64_64_32_011_1.7447_0.4447_1.8652_0.4109.hdf5
Epoch 12/20
Epoch 00012: loss improved from 1.74469 to 1.72684, saving model to weights/three_64_64_64_32_012_1.7268_0.4513_1.8625_0.4149.hdf5
Epoch 13/20
Epoch 00013: loss improved from 1.72684 to 1.70933, saving model to weights/three_64_64_64_32_013_1.7093_0.4564_1.8432_0.4210.hdf5
Epoch 14/20
Epoch 00014: loss improved from 1.70933 to 1.69472, saving model to weights/three_64_64_64_32_014_1.6947_0.4596_1.8543_0.4171.hdf5
Epoch 15/20
Epoch 00015: loss improved from 1.69472 to 1.67790, saving model to weights/three_64_64_64_32_015_1.6779_0.4671_1.8681_0.4140.hdf5
Epoch 16/20
Epoch 00016: loss improved from 1.67790 to 1.66758, saving model to weights/three_64_64_64_32_016_1.6676_0.4704_1.8440_0.4204.hdf5
Test loss: 1.8439889444	Test acc: 0.420351445259
Train on 88944 samples, validate on 29649 samples
Epoch 1/20
Epoch 00001: loss improved from inf to 2.33195, saving model to weights/three_256_64_512_32_001_2.3320_0.2859_2.1156_0.3377.hdf5
Epoch 2/20
Epoch 00002: loss improved from 2.33195 to 2.01261, saving model to weights/three_256_64_512_32_002_2.0126_0.3623_2.1168_0.3338.hdf5
Epoch 3/20
Epoch 00003: loss improved from 2.01261 to 1.85429, saving model to weights/three_256_64_512_32_003_1.8543_0.4107_1.8909_0.4045.hdf5
Epoch 4/20
Epoch 00004: loss improved from 1.85429 to 1.73014, saving model to weights/three_256_64_512_32_004_1.7301_0.4480_1.8890_0.4046.hdf5
Epoch 5/20
Epoch 00005: loss improved from 1.73014 to 1.61162, saving model to weights/three_256_64_512_32_005_1.6116_0.4825_1.9245_0.3955.hdf5
Epoch 6/20
Epoch 00006: loss improved from 1.61162 to 1.52839, saving model to weights/three_256_64_512_32_006_1.5284_0.5098_1.8823_0.4153.hdf5
Epoch 7/20
Epoch 00007: loss improved from 1.52839 to 1.48601, saving model to weights/three_256_64_512_32_007_1.4860_0.5231_1.8678_0.4281.hdf5
Epoch 8/20
Epoch 00008: loss improved from 1.48601 to 1.48587, saving model to weights/three_256_64_512_32_008_1.4859_0.5226_1.9286_0.4077.hdf5
Epoch 9/20
Epoch 00009: loss did not improve
Epoch 10/20
Epoch 00010: loss did not improve
Test loss: 2.07538824861	Test acc: 0.367971938359
Train on 88944 samples, validate on 29649 samples
Epoch 1/20
Epoch 00001: loss improved from inf to 2.41350, saving model to weights/three_256_64_128_64_001_2.4135_0.2642_2.2151_0.3047.hdf5
Epoch 2/20
Epoch 00002: loss improved from 2.41350 to 2.12489, saving model to weights/three_256_64_128_64_002_2.1249_0.3313_2.0572_0.3472.hdf5
Epoch 3/20
Epoch 00003: loss improved from 2.12489 to 1.98064, saving model to weights/three_256_64_128_64_003_1.9806_0.3680_1.9483_0.3762.hdf5
Epoch 4/20
Epoch 00004: loss improved from 1.98064 to 1.88651, saving model to weights/three_256_64_128_64_004_1.8865_0.3950_1.9186_0.3894.hdf5
Epoch 5/20
Epoch 00005: loss improved from 1.88651 to 1.82083, saving model to weights/three_256_64_128_64_005_1.8208_0.4158_1.8723_0.3996.hdf5
Epoch 6/20
Epoch 00006: loss improved from 1.82083 to 1.76503, saving model to weights/three_256_64_128_64_006_1.7650_0.4315_1.8460_0.4059.hdf5
Epoch 7/20
Epoch 00007: loss improved from 1.76503 to 1.71787, saving model to weights/three_256_64_128_64_007_1.7179_0.4453_1.8313_0.4154.hdf5
Epoch 8/20
Epoch 00008: loss improved from 1.71787 to 1.67092, saving model to weights/three_256_64_128_64_008_1.6709_0.4606_1.8521_0.4021.hdf5
Epoch 9/20
Epoch 00009: loss improved from 1.67092 to 1.62977, saving model to weights/three_256_64_128_64_009_1.6298_0.4732_1.8419_0.4141.hdf5
Epoch 10/20
Epoch 00010: loss improved from 1.62977 to 1.58613, saving model to weights/three_256_64_128_64_010_1.5861_0.4865_1.8267_0.4239.hdf5
Epoch 11/20
Epoch 00011: loss improved from 1.58613 to 1.54720, saving model to weights/three_256_64_128_64_011_1.5472_0.4985_1.8286_0.4208.hdf5
Epoch 12/20
Epoch 00012: loss improved from 1.54720 to 1.51059, saving model to weights/three_256_64_128_64_012_1.5106_0.5091_1.8150_0.4300.hdf5
Epoch 13/20
Epoch 00013: loss improved from 1.51059 to 1.47263, saving model to weights/three_256_64_128_64_013_1.4726_0.5233_1.8287_0.4315.hdf5
Epoch 14/20
Epoch 00014: loss improved from 1.47263 to 1.43689, saving model to weights/three_256_64_128_64_014_1.4369_0.5346_1.8296_0.4277.hdf5
Epoch 15/20
Epoch 00015: loss improved from 1.43689 to 1.40662, saving model to weights/three_256_64_128_64_015_1.4066_0.5440_1.8311_0.4294.hdf5
Test loss: 1.83114928927	Test acc: 0.429424263904
Train on 88944 samples, validate on 29649 samples
Epoch 1/20
Epoch 00001: loss improved from inf to 2.55734, saving model to weights/three_32_256_64_128_001_2.5573_0.2465_2.3765_0.2808.hdf5
Epoch 2/20
Epoch 00002: loss improved from 2.55734 to 2.28772, saving model to weights/three_32_256_64_128_002_2.2877_0.2979_2.2195_0.3106.hdf5
Epoch 3/20
Epoch 00003: loss improved from 2.28772 to 2.14530, saving model to weights/three_32_256_64_128_003_2.1453_0.3305_2.1004_0.3351.hdf5
Epoch 4/20
Epoch 00004: loss improved from 2.14530 to 2.03971, saving model to weights/three_32_256_64_128_004_2.0397_0.3534_2.0212_0.3584.hdf5
Epoch 5/20
Epoch 00005: loss improved from 2.03971 to 1.95808, saving model to weights/three_32_256_64_128_005_1.9581_0.3776_1.9806_0.3716.hdf5
Epoch 6/20
Epoch 00006: loss improved from 1.95808 to 1.88859, saving model to weights/three_32_256_64_128_006_1.8886_0.3969_1.9488_0.3774.hdf5
Epoch 7/20
Epoch 00007: loss improved from 1.88859 to 1.82820, saving model to weights/three_32_256_64_128_007_1.8282_0.4153_1.9159_0.3925.hdf5
Epoch 8/20
Epoch 00008: loss improved from 1.82820 to 1.76968, saving model to weights/three_32_256_64_128_008_1.7697_0.4320_1.9124_0.3955.hdf5
Epoch 9/20
Epoch 00009: loss improved from 1.76968 to 1.71249, saving model to weights/three_32_256_64_128_009_1.7125_0.4503_1.8839_0.3991.hdf5
Epoch 10/20
Epoch 00010: loss improved from 1.71249 to 1.65627, saving model to weights/three_32_256_64_128_010_1.6563_0.4676_1.8828_0.4073.hdf5
Epoch 11/20
Epoch 00011: loss improved from 1.65627 to 1.60144, saving model to weights/three_32_256_64_128_011_1.6014_0.4860_1.8838_0.4090.hdf5
Epoch 12/20
Epoch 00012: loss improved from 1.60144 to 1.54769, saving model to weights/three_32_256_64_128_012_1.5477_0.5010_1.8779_0.4097.hdf5
Epoch 13/20
Epoch 00013: loss improved from 1.54769 to 1.49484, saving model to weights/three_32_256_64_128_013_1.4948_0.5192_1.8848_0.4150.hdf5
Epoch 14/20
Epoch 00014: loss improved from 1.49484 to 1.44181, saving model to weights/three_32_256_64_128_014_1.4418_0.5336_1.8887_0.4167.hdf5
Epoch 15/20
Epoch 00015: loss improved from 1.44181 to 1.38931, saving model to weights/three_32_256_64_128_015_1.3893_0.5547_1.9284_0.4072.hdf5
Test loss: 1.928418992	Test acc: 0.407197544621
Train on 88944 samples, validate on 29649 samples
Epoch 1/20
Epoch 00001: loss improved from inf to 2.35780, saving model to weights/three_512_128_256_64_001_2.3578_0.2785_2.1432_0.3196.hdf5
Epoch 2/20
Epoch 00002: loss improved from 2.35780 to 2.04565, saving model to weights/three_512_128_256_64_002_2.0456_0.3518_2.0041_0.3533.hdf5
Epoch 3/20
Epoch 00003: loss improved from 2.04565 to 1.90906, saving model to weights/three_512_128_256_64_003_1.9091_0.3890_1.9396_0.3789.hdf5
Epoch 4/20
Epoch 00004: loss improved from 1.90906 to 1.80972, saving model to weights/three_512_128_256_64_004_1.8097_0.4174_1.8829_0.4027.hdf5
Epoch 5/20
Epoch 00005: loss improved from 1.80972 to 1.71818, saving model to weights/three_512_128_256_64_005_1.7182_0.4453_1.8668_0.4075.hdf5
Epoch 6/20
Epoch 00006: loss improved from 1.71818 to 1.63545, saving model to weights/three_512_128_256_64_006_1.6354_0.4713_1.8106_0.4235.hdf5
Epoch 7/20
Epoch 00007: loss improved from 1.63545 to 1.55416, saving model to weights/three_512_128_256_64_007_1.5542_0.4959_1.7930_0.4366.hdf5
Epoch 8/20
Epoch 00008: loss improved from 1.55416 to 1.47298, saving model to weights/three_512_128_256_64_008_1.4730_0.5208_1.8007_0.4350.hdf5
Epoch 9/20
Epoch 00009: loss improved from 1.47298 to 1.40234, saving model to weights/three_512_128_256_64_009_1.4023_0.5439_1.8171_0.4387.hdf5
Epoch 10/20
Epoch 00010: loss improved from 1.40234 to 1.33304, saving model to weights/three_512_128_256_64_010_1.3330_0.5669_1.8213_0.4441.hdf5
Test loss: 1.82133922305	Test acc: 0.444095922307
BEST MODEL:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_13 (GRU)                 (None, 100, 512)          789504    
_________________________________________________________________
gru_14 (GRU)                 (None, 100, 128)          246144    
_________________________________________________________________
gru_15 (GRU)                 (None, 256)               295680    
_________________________________________________________________
dense_5 (Dense)              (None, 50)                12850     
=================================================================
Total params: 1,344,178
Trainable params: 1,344,178
Non-trainable params: 0
_________________________________________________________________
None
BEST_RUN:
{'units_6': 4, 'units_3': 2, 'numgrulayers': 0, 'units_4': 3, 'units_1': 2, 'batch_size': 1, 'units_5': 3, 'units_2': 3, 'units': 4, 'units_7': 3}

